{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import json\n",
    "import requests \n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (15,5)\n",
    "plt.rcParams['axes.grid'] = 'on'\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "from helpers import *\n",
    "from dataset import DreemDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dreem Case study\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all processings defined in the previous notebook, we can create a dataset object that will load raw records, process and cut them into 30sec data points. The init is a bit long since all the processing is done here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset = 5467\n",
      "Sample shape is (7, 1500)\n"
     ]
    }
   ],
   "source": [
    "# Paths to data\n",
    "PATH_TO_DATA = Path(\"data/h5/\")\n",
    "PATH_TO_PROCESSED_DATA = Path(\"data/h5_processed/\")\n",
    "PATH_TO_HYPNOGRAM = Path(\"data/hypnograms/\")\n",
    "\n",
    "# List of expected records\n",
    "records_list =  [\n",
    "\"8e0bf011-1db6-46fa-a3cd-496e60c0de6f\",\n",
    "\"d8a9babd-8454-42e9-9286-eb66c996d3e6\",\n",
    "\"c5080eac-a388-4b1f-818f-a7f902fe4c06\",\n",
    "\"62492470-d4d5-4dee-8030-80cca44fb002\",\n",
    "\"87748119-6fff-45d2-9219-888532fb7efd\",\n",
    "\"9bd9224a-bbdf-46c2-a494-3bbfcfd7e776\",\n",
    "\"8f3dc41c-df99-4a5f-82cf-6b9f6e265b92\"\n",
    "]\n",
    "\n",
    "# Instantiating the data set will load and process records into samples stored in PATH_TO_PROCESSED_DATA\n",
    "dataset = DreemDataset(\n",
    "    records_list, PATH_TO_DATA, PATH_TO_HYPNOGRAM, Path(PATH_TO_PROCESSED_DATA))\n",
    "print(f\"Number of samples in dataset = {len(dataset)}\")\n",
    "print(f\"Sample shape is {dataset[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now a sample is a (7, 1500) matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define learning framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep one record as validation and the remaining in the training process. The latter will be splitted into a train and test set to monitor networks learning process.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=len(train_idx), sampler=train_sampler)\n",
    "tdata = next(iter(train_loader))\n",
    "channel_means = [tdata[0][:, i, :].mean().item() for i in range(7)]\n",
    "channel_stds = [tdata[0][:, i, :].std().item() for i in range(7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in a context of multiclass classification (C=5). Our choice of metric must include the class imbalance information. \n",
    "- We can analyze the performance of a model precisely by looking at traditional metrics (confusion_matrix  -> precision, recall, roc_curve, balanced accuracy, etc.) for each class value.\n",
    "- We can provide a global metric with a weighted average of per-class metrics (balanced accuracy or f1 score with average = micro to take into account class imbalances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define learning model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many possible ways of classifying EEG data, we will go with the implementation from [here](https://arxiv.org/pdf/1707.03321.pdf) (at least a part of it). The paper proposes a feature extractor network that is used to make predictions without temporal context. The features obtained with that trained network are then used to perform temporal sleep stage classification (by integrating features from adjacent time segments in the input) (this part is not done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1500\n",
    "# C = 7\n",
    "\n",
    "class BasicModel(nn.Module):\n",
    "\n",
    "    def __init__(self, C=7):\n",
    "        super().__init__()   \n",
    "        \n",
    "        # First conv layer\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = C, out_channels = C, \n",
    "            kernel_size = (C, 1), stride = (1, 1), \n",
    "            padding = 'same')  # VALID in paper, mistake ?\n",
    "        # Activation ??\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = 1, \n",
    "            out_channels = 8, \n",
    "            kernel_size = (1, 25), \n",
    "            stride = (1, 1), \n",
    "            padding = 'same')\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = (1, 6), stride = (1, 6))\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels = 8, \n",
    "            out_channels = 8, \n",
    "            kernel_size = (1, 25), \n",
    "            stride = (1, 1), \n",
    "            padding = 'same')\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size = (1, 6), stride = (1, 6))\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.layer_out = nn.Linear(C*(T//36)*8, 5) \n",
    "        # self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Start with (Batch_size, C, T)\n",
    "        x = x[:, :, :, None] # (Batch_size, C, T, 1)\n",
    "        # print(\"Expand\", x.requires_grad)\n",
    "        x = self.conv1(x) # (Batch_size, C, T, 1)\n",
    "        # print(\"Conv1\", x.requires_grad)\n",
    "        x = torch.permute(x, (0, 3, 1, 2)) # (Batch_size, 1, C, T)\n",
    "        # print(\"Permute\", x.requires_grad)\n",
    "        x = self.conv2(x) # (Batch_size, 8, C, T)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool3(x) # (Batch_size, 8, C, T//6)\n",
    "        x = self.conv3(x) # (Batch_size, 8, C, T//6)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool4(x) # (Batch_size, 8, C, T//36)\n",
    "        x = torch.flatten(x, start_dim=1) # (Batch_size, 8*C*(T//36))\n",
    "        # print(x.requires_grad)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x) # (B, 5)\n",
    "        # No softmax since we are using CrossEntropyLoss which \n",
    "        # expects logits as the model output not probabilities coming from softmax.\n",
    "        # print(x.requires_grad)\n",
    "        return x\n",
    "    \n",
    "# initialize weights as indicated in the paper\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(0.0, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the process defined in the paper. Since our classes are imbalanced, we will perform a custom batch sampling that uses a stratified splitting to maintain class ratios in the various batches.\n",
    "\n",
    "The metric that we will use, as proposed in the paper, will be balanced accuracy (every class will have similar impact on the final score, this will account for the imbalance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 2.05559 (acc: 0.34417) | Test Loss: 2.18590 (acc: 0.31079)\n",
      "Epoch 002: | Train Loss: 0.65304 (acc: 0.57516) | Test Loss: 2.66313 (acc: 0.35466)\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(action=\"ignore\", message=\"y_pred contains classes not in y_true\")\n",
    "\n",
    "n_epochs = 2\n",
    "model = BasicModel().double()\n",
    "model.apply(weights_init)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_epoch_losses = []\n",
    "test_epoch_losses = []\n",
    "\n",
    "writer = SummaryWriter()\n",
    "n_iter = 0\n",
    "targets = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    \n",
    "    # TODO stratified batches ?\n",
    "    train_idx, test_idx = train_test_split(np.arange(len(targets)), stratify=targets, test_size=0.2, shuffle=True)  \n",
    "    test_sampler = torch.utils.data.RandomSampler(test_idx)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=test_sampler)\n",
    "\n",
    "    train_idx = train_idx[:len(train_idx)-len(train_idx)%128]\n",
    "    skf = StratifiedKFold(n_splits=34)\n",
    "    splits = skf.split(train_idx, targets[train_idx])\n",
    "    \n",
    "    for _, batch_idx in splits:\n",
    "        \n",
    "        train_sampler = torch.utils.data.RandomSampler(batch_idx)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=len(batch_idx), sampler=train_sampler)\n",
    "        X_train_batch, y_train_batch = next(iter(train_loader))\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            X_train_batch = Variable(X_train_batch, requires_grad=True)\n",
    "\n",
    "            # Get prediction\n",
    "            y_train_pred = model(X_train_batch)\n",
    "\n",
    "            # Calculate loss\n",
    "            train_loss = criterion(y_train_pred, torch.tensor(get_one_hot_encoding(y_train_batch)))\n",
    "\n",
    "            # Backpropagate\n",
    "            train_loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record batch loss\n",
    "            train_epoch_loss += train_loss.item()\n",
    "            \n",
    "            train_acc = balanced_accuracy_score(y_train_batch, torch.argmax(y_train_pred, axis=1))\n",
    "            train_epoch_acc+=train_acc\n",
    "            \n",
    "        writer.add_scalar('Loss/train', train_loss.item(), n_iter)\n",
    "        writer.add_scalar('Acc/train', train_acc, n_iter)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_epoch_loss = 0\n",
    "            test_epoch_acc = 0\n",
    "            for X_test_batch, y_test_batch in test_loader:     \n",
    "                \n",
    "                y_test_pred = model(X_test_batch)\n",
    "                \n",
    "                test_loss = criterion(y_test_pred, torch.tensor(get_one_hot_encoding(y_test_batch)))\n",
    "                test_epoch_loss += test_loss.item()\n",
    "                \n",
    "                test_acc = balanced_accuracy_score(y_test_batch, torch.argmax(y_test_pred, axis=1))\n",
    "                \n",
    "                test_epoch_acc+=test_acc\n",
    "                \n",
    "            writer.add_scalar('Loss/test_epoch', test_epoch_loss/len(test_loader), n_iter)\n",
    "            writer.add_scalar('Acc/test_epoch', test_epoch_acc/len(test_loader), n_iter)\n",
    "                \n",
    "        n_iter +=1\n",
    "    \n",
    "    train_loss_str = f'Train Loss: {train_epoch_loss/34:.5f}'\n",
    "    train_acc_str = f'acc: {train_epoch_acc/34:.5f}'\n",
    "      \n",
    "    test_loss_str = f'Test Loss: {test_epoch_loss/len(test_loader):.5f}'\n",
    "    test_acc_str = f'acc: {test_epoch_acc/len(test_loader):.5f}'\n",
    "    \n",
    "    print(f'Epoch {epoch+1:03}: | {train_loss_str} ({train_acc_str}) | {test_loss_str} ({test_acc_str})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training monitored on tensorboard, test loss goes up after the first two epochs so we stop the training there. We can use this network as it is but the idea is to push forward with the implementation proposed in [this paper](https://arxiv.org/pdf/1707.03321.pdf). Overall performance is bad, we can see that the training performance increases so the model learns, however it overfits very quickly. We will not go further on this subject, by lack of time. Some ideas : \n",
    "- Finish implementing paper with the second model\n",
    "- Analyze results per class, identify where the selected model fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/saved_model_state_dict.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test API call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run *app.py* with flask before executing this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'http://127.0.0.1:5000/hypnogram'\n",
    "record_id = '87748119-6fff-45d2-9219-888532fb7efd'\n",
    "\n",
    "r = requests.get(url = f'{domain}/{record_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEEP', 'DEEP', 'DEEP', 'DEEP', 'N2', 'DEEP', 'DEEP', 'N2', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'N2', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'N2', 'DEEP', 'N2', 'N2', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'N2', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'N2', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'N2', 'N2', 'DEEP', 'DEEP', 'N2', 'N2', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'N2', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'N2', 'N2', 'DEEP', 'DEEP', 'N2', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP', 'DEEP']\n"
     ]
    }
   ],
   "source": [
    "print(r.json()[:100]) # Returns a lot of DEEPs because the model is bad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
